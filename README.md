# Uniform Stability of Adam Algorithm and its Effect on The Generalization Error of Deep Neural Networks
## Abstract
* We connected the Lipschitz constant of a loss function to the generalization error of deep learning models trained by Adam and AdamW optimizers under the uniform stability theory.
* Using the generalziation bounds that we proved, we proposed a novel loss function for training deep classification models to improve generalization performance and overcome the over-fitting issue.
* We assessed our theoretical results in age estimation.
* We trained deep neural networks using our new loss function in image and node classification problems in order to stabilize the output models and increase their accuarcy.
## Paper
* An original paper contains the theorems, proofs, and experiments on age estimation has been accepted for publication in Amirkabir Journal of Mathematics and Computing (AJMC);
* Article on the journal website: https://ajmc.aut.ac.ir/article_5213.html;
* PDF on arxiv: https://arxiv.org/abs/2303.16464.
